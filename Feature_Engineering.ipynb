{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implemented so far:\n",
    "\n",
    "**Feature engineering**\n",
    "\n",
    "- New features created:\n",
    "    -  Wildernes Density Features\n",
    "    - Euclidian distances\n",
    "- Other:\n",
    "    - merge all soil types into 1 col\n",
    "\n",
    "\n",
    "**Feature Selection**\n",
    "- Remove features weakly correlated (<0.005) to target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load the training data\n",
    "train_path = '../data/train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "# load the testing data\n",
    "test_path = '../data/test-full.csv'\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Wilderness Density Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a DataFrame to hold the counts of each cover type within each wilderness area\n",
    "wilderness_cover_counts = train.groupby('Cover_Type').agg({\n",
    "    'Wilderness_Area1': 'sum',\n",
    "    'Wilderness_Area2': 'sum',\n",
    "    'Wilderness_Area3': 'sum',\n",
    "    'Wilderness_Area4': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Now, we calculate the density for each cover type within each wilderness area\n",
    "total_counts = train[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].sum()\n",
    "wilderness_cover_density = wilderness_cover_counts.set_index('Cover_Type').div(total_counts)\n",
    "\n",
    "# Function to apply the density-based feature to each row\n",
    "def apply_density_feature(row):\n",
    "    cover_type = row['Cover_Type']\n",
    "    for i in range(1, 5):\n",
    "        wilderness_area = f'Wilderness_Area{i}'\n",
    "        if row[wilderness_area] == 1:  # If the observation is in wilderness area i\n",
    "            return wilderness_cover_density.loc[cover_type, wilderness_area]\n",
    "\n",
    "# Apply the function to create the new density-based feature\n",
    "train['Cover_Type_Density'] = train.apply(apply_density_feature, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create euclidian distance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset after adding new features: (15120, 61)\n"
     ]
    }
   ],
   "source": [
    "# Calculate new features based on mean distances\n",
    "train['Mean_Elevation_Vertical_Distance_Hydrology'] = (train['Elevation'] + train['Vertical_Distance_To_Hydrology']) / 2\n",
    "train['Mean_Distance_Hydrology_Firepoints'] = (train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Fire_Points']) / 2\n",
    "train['Mean_Distance_Hydrology_Roadways'] = (train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways']) / 2\n",
    "train['Mean_Distance_Firepoints_Roadways'] = (train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Roadways']) / 2\n",
    "\n",
    "print(f\"Shape of the dataset after adding new features: {train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove low correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed features with low correlation to 'Cover_Type': ['Horizontal_Distance_To_Hydrology', 'Soil_Type11', 'Soil_Type26', 'Soil_Type34']\n",
      "Shape of the dataset before removal: (15120, 57)\n",
      "Shape of the dataset after removal: (15120, 57)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'train' is your DataFrame and includes both features and the target variable 'Cover_Type'\n",
    "correlation_matrix = train.corr()  # Compute the correlation matrix\n",
    "\n",
    "# Calculate the correlation of each feature with 'Cover_Type'\n",
    "feature_correlation = correlation_matrix['Cover_Type'].drop('Cover_Type')  # Exclude self-correlation\n",
    "\n",
    "# Decide on a threshold for low correlation (example: below 0.02 in absolute value)\n",
    "low_correlation_features = feature_correlation[abs(feature_correlation) < 0.005].index.tolist()\n",
    "\n",
    "# Drop these low correlation features from your dataset\n",
    "train = train.drop(low_correlation_features, axis=1)\n",
    "\n",
    "print(f\"Removed features with low correlation to 'Cover_Type': {low_correlation_features}\")\n",
    "print(f\"Shape of the dataset before removal: {train.shape}\")\n",
    "print(f\"Shape of the dataset after removal: {train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Soil Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract soil type columns\n",
    "soil_columns = [col for col in train.columns if col.startswith('Soil_Type')]\n",
    "# Combine the soil type columns into a single feature by identifying the active soil type\n",
    "train['Soil_Type_Combined'] = train[soil_columns].idxmax(axis=1).str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Drop the original soil type columns\n",
    "train = train.drop(soil_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Scores\n",
    "- No feature engineering: 86.44%\n",
    "- Merging Soil Types: 86.87%\n",
    "- Merging Soil Types + Removing Low Corr.: 86.28%\n",
    "- Merging Soil Types + Removing Low Corr. + Adding Distances: 87.07%\n",
    "- Merging Soil Types + Removing Low Corr. + Adding Distances + Adding Wilderness Density: 99% (??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973544973544973"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "X = train.drop(['Id', 'Cover_Type'], axis=1)\n",
    "y = train['Cover_Type']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
